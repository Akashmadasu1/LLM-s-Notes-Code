{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0uUO3Nu+xKKd2kJ/uSmB4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akashmadasu1/LLM-s-Notes-Code/blob/main/LLMs_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lesson 1: What is an LLM and how it works**"
      ],
      "metadata": {
        "id": "LQvjxiM7nZ8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Big Picture\n",
        "\n",
        "You already know in ML we train models to map inputs → outputs (like predicting prices).\n",
        "Now imagine doing that for language:\n",
        "\n",
        "Input = text (a sentence)\n",
        "\n",
        "Output = text (the next word, sentence, or answer)\n",
        "\n",
        "\n",
        "That’s what an LLM (Large Language Model) does — it’s a neural network trained on huge text data to predict the next word.\n",
        "Because it’s trained on billions of examples, it learns grammar, meaning, reasoning, and even style.\n",
        "\n",
        "So at its core:\n",
        "\n",
        "LLM = giant text-prediction engine that’s become so good it feels like it understands.\n",
        "\n",
        "Step 2: How it works — simplified flow\n",
        "\n",
        "Text → Tokens:\n",
        "Every word/piece of text is converted into numbers.\n",
        "\n",
        "Tokens → Embeddings:\n",
        "Each token becomes a vector (list of numbers) representing meaning.\n",
        "\n",
        "Transformer model:\n",
        "Uses self-attention to understand context — which words relate to which.\n",
        "\n",
        "Prediction:\n",
        "It predicts the most likely next token, one by one, to form coherent sentences.\n",
        "\n",
        "Step 3: A simple example\n",
        "\n",
        "Input: \"The sky is\"\n",
        "Model predicts → \"blue\" (because that’s most likely next in training data).\n",
        "But if context changes — \"At night the sky is\" → \"dark\" —\n",
        "the model uses context through attention to adjust the prediction."
      ],
      "metadata": {
        "id": "XJ99-AcVnkq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:- The LLM’s main goal is to predict the next word (token) based on the previous context it’s seen during training.\n",
        "\n",
        "And because it’s trained on massive amounts of data, it can do more than just complete sentences — it can answer questions, summarize, translate, reason, and even write code."
      ],
      "metadata": {
        "id": "dpR9Ex0qn9My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Step: Tokenization & Embeddings\n",
        "\n",
        "Before an LLM can “understand” text, it must convert words into numbers — because neural networks can only process numbers.\n",
        "This process happens in two steps:\n",
        "\n",
        "1.Tokenization – breaking text into small chunks (tokens).\n",
        "Example:"
      ],
      "metadata": {
        "id": "gETlEGc3oMNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"ChatGPT is powerful\"\n",
        "→ [\"Chat\", \"G\", \"PT\", \"is\", \"powerful\"]"
      ],
      "metadata": {
        "id": "q3nGNSI3nkQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Embedding – converting each token into a vector (a list of numbers) that represents meaning.\n",
        "Example (simplified):"
      ],
      "metadata": {
        "id": "TRo3ivaLofPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"Chat\" → [0.12, 0.84, -0.33, ...]"
      ],
      "metadata": {
        "id": "q7M7kKK5odPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These vectors capture relationships — words with similar meanings have similar embeddings."
      ],
      "metadata": {
        "id": "f4kY1O3ZosDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practice**\n",
        "\n",
        "Let’s do a small hands-on demo to see tokenization and embeddings in action — this will help you see how text turns into numbers inside an LLM."
      ],
      "metadata": {
        "id": "zpMf3GohotH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Setup**\n",
        "\n",
        "We’ll use Hugging Face Transformers — the most popular library for working with LLMs."
      ],
      "metadata": {
        "id": "35mtb-4Vp8pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Transformers library (only once)\n",
        "!pip install transformers\n",
        "\n",
        "# Import tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Step 1: Choose a small model (DistilBERT)\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# Step 2: Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Step 3: Input text\n",
        "text = \"ChatGPT is powerful\"\n",
        "\n",
        "# Step 4: Tokenize the text\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Step 5: Convert tokens into IDs (numbers)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Step 6: Create embeddings\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "embeddings = outputs.last_hidden_state\n",
        "\n",
        "print(\"Embeddings shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "id": "OYhJZu3mo3eV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}